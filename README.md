# gradient-decent-in-linear-regression

## How Gradient Descent Works in Linear Regression:

1. **Initialize Parameters**: Begin by assigning random initial values for the slope \(m\) and the intercept \(b\).

2. **Compute the Cost Function**: Calculate the error between the predicted values and the actual values using a cost function like Mean Squared Error (MSE):
3. **Calculate the Gradient**: Determine the gradient of the cost function with respect to \(m\) (slope) and \(b\) (intercept). These gradients reveal how the cost changes when the parameters are adjusted:
4. **Update the Parameters**: Adjust \(m\) and \(b\) in the direction that reduces the cost, based on the gradients:
5. **Iterate**: Repeat the process until the cost function converges, meaning further updates yield negligible changes in the cost.
